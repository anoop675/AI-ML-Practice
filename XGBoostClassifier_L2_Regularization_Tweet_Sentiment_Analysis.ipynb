{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rS-rUl87h9Ab"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkHZ4Ei-jdiD",
        "outputId": "fa03277d-f61c-4442-9b6f-79edc9b9c4ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xgboost\n",
            "  Downloading xgboost-3.1.1-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n",
            "Collecting nvidia-nccl-cu12 (from xgboost)\n",
            "  Downloading nvidia_nccl_cu12-2.28.7-py3-none-manylinux_2_18_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.3)\n",
            "Downloading xgboost-3.1.1-py3-none-manylinux_2_28_x86_64.whl (115.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.28.7-py3-none-manylinux_2_18_x86_64.whl (296.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.8/296.8 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nccl-cu12, xgboost\n",
            "Successfully installed nvidia-nccl-cu12-2.28.7 xgboost-3.1.1\n"
          ]
        }
      ],
      "source": [
        "import re #importing regex\n",
        "import csv #csv reader\n",
        "import numpy as np\n",
        "#from sklearn.svm import LinearSVC #assumes data is linearly separable, and faster for high-dimensional sparse text features\n",
        "!pip install xgboost #download xgboost package first\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from nltk.classify import SklearnClassifier\n",
        "from sklearn.metrics import precision_recall_fscore_support # to report on precision and recall\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import FeatureUnion, Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer #importing WordNetLemmatizer to lemmatize words into lemmas\n",
        "from nltk.corpus import stopwords #importing all common stop words\n",
        "from scipy import sparse   # used to convert dense numeric features into sparse matrices for stacking\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV  # used in cross validation / grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPpbHAMLcOIY",
        "outputId": "e5ab0ca2-fcd9-4aec-f995-71e76b1a295e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "nltk.download('wordnet') #run only once\n",
        "nltk.download('stopwords') #run only once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lBEW-WGgcSa0"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "k2Ibg_aWjdiF"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "class LexiconFeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Custom transformer to add features from external opinion lexicons (Bing Liu). It adds [Positive_Count, Negative_Count, Polarity_Score, Intense_Punctuation_Count].\n",
        "    \"\"\"\n",
        "    def __init__(self, pos_file=None, neg_file=None, try_download=True):\n",
        "        self.pos_words = set()\n",
        "        self.neg_words = set()\n",
        "        self._tried_to_download = False\n",
        "\n",
        "        # 1) Try reading local files if provided\n",
        "        if pos_file:\n",
        "            try:\n",
        "                with open(pos_file, 'r', encoding='utf-8', errors='ignore') as pf:\n",
        "                    self.pos_words = set(word.strip().lower() for word in pf if word.strip() and not word.strip().startswith(';'))\n",
        "            except FileNotFoundError:\n",
        "                pass\n",
        "        if neg_file:\n",
        "            try:\n",
        "                with open(neg_file, 'r', encoding='utf-8', errors='ignore') as nf:\n",
        "                    self.neg_words = set(word.strip().lower() for word in nf if word.strip() and not word.strip().startswith(';'))\n",
        "            except FileNotFoundError:\n",
        "                pass\n",
        "\n",
        "        # 2) If either lexicon is missing and try_download is True, attempt to fetch from a public raw mirror\n",
        "        if try_download and (not self.pos_words or not self.neg_words):\n",
        "            self._tried_to_download = True\n",
        "            try:\n",
        "                pos_url = \"https://gist.githubusercontent.com/mkulakowski2/4289437/raw/positive-words.txt\"\n",
        "                neg_url = \"https://gist.githubusercontent.com/mkulakowski2/4289441/raw/negative-words.txt\"\n",
        "\n",
        "                if not self.pos_words:\n",
        "                    rpos = requests.get(pos_url, timeout=10)\n",
        "                    if rpos.status_code == 200:\n",
        "                        lines = rpos.text.splitlines()\n",
        "                        self.pos_words = set(w.strip().lower() for w in lines if w.strip() and not w.strip().startswith(';'))\n",
        "\n",
        "                if not self.neg_words:\n",
        "                    rneg = requests.get(neg_url, timeout=10)\n",
        "                    if rneg.status_code == 200:\n",
        "                        lines = rneg.text.splitlines()\n",
        "                        self.neg_words = set(w.strip().lower() for w in lines if w.strip() and not w.strip().startswith(';'))\n",
        "            except Exception:\n",
        "                # network error or requests not installed: fall through to fallback\n",
        "                pass\n",
        "\n",
        "        if (not self.pos_words) or (not self.neg_words):\n",
        "            if self._tried_to_download:\n",
        "                print(\"Could not download the positive/negative lexicon files.\")\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        num_texts = len(X)\n",
        "        num_features = 4\n",
        "        features = np.zeros([num_texts, num_features])  # pos_count, neg_count, polarity_score, intense_punct_count\n",
        "\n",
        "        for i, text in enumerate(X):\n",
        "            words = re.findall(r'\\b\\w+\\b', text.lower())  # simple word tokenization\n",
        "            intense_punct_count = len(re.findall(r'(\\!{2,}|\\?{2,})', text))  # count repeated ! or ?\n",
        "\n",
        "            pos_count = sum(1 for word in words if word in self.pos_words)\n",
        "            neg_count = sum(1 for word in words if word in self.neg_words)\n",
        "            polarity_score = pos_count - neg_count\n",
        "\n",
        "            features[i] = [pos_count, neg_count, polarity_score, intense_punct_count]\n",
        "\n",
        "        # convert to sparse CSR so FeatureUnion can hstack with TF-IDF sparse matrices efficiently\n",
        "        return sparse.csr_matrix(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JaoYWHIrfp3d"
      },
      "outputs": [],
      "source": [
        "def to_feature_vector(data, preprocessor_func, train=False):\n",
        "    \"\"\"\n",
        "    Updated feature extraction function using FeatureUnion and TF-IDF/N-grams.\n",
        "    If 'train' is True, it returns the fitted FeatureUnion object for use in 'train_classifier'.\n",
        "    If 'train' is False, it returns the feature matrix (X) transformed by the fitted union.\n",
        "    \"\"\"\n",
        "    # Word-level TF-IDF (uses your pre_process tokenizer)\n",
        "    '''ngram_vectorizer = TfidfVectorizer( #TF-IDF features as n-grams\n",
        "        tokenizer=lambda x: pre_process(x, lemmatizer, stop_words), # tokenizing each sentence using the tools passed\n",
        "        ngram_range=(1, 2), #unigrams and bigrams (kept small to be robust)\n",
        "        sublinear_tf=True,\n",
        "        max_df=0.85,\n",
        "        min_df=3,\n",
        "        max_features=10000 #vocabulary size limited to 10000 features\n",
        "    )\n",
        "\n",
        "    # Character-level TF-IDF: helps with misspellings, elongations (soooo), emotive punctuation patterns\n",
        "    char_vectorizer = TfidfVectorizer(\n",
        "        analyzer='char',\n",
        "        ngram_range=(3,5),\n",
        "        sublinear_tf=True,\n",
        "        max_df=0.85,\n",
        "        min_df=3,\n",
        "        max_features=5000\n",
        "    )'''\n",
        "    ngram_vectorizer = TfidfVectorizer( #TF-IDF features as n-grams\n",
        "        tokenizer=lambda x: pre_process(x, lemmatizer, stop_words), # tokenizing each sentence using the tools passed\n",
        "        ngram_range=(1, 3), #include trigrams (unigrams, bigrams, trigrams)\n",
        "        sublinear_tf=True,\n",
        "        max_df=0.8,   #slightly stricter to remove very common tokens\n",
        "        min_df=2,     #lower min_df to include rarer but informative tokens\n",
        "        max_features=15000 # slightly larger vocab to accommodate trigrams\n",
        "    )\n",
        "\n",
        "    char_vectorizer = TfidfVectorizer(\n",
        "        analyzer='char',\n",
        "        ngram_range=(3, 5),\n",
        "        sublinear_tf=True,\n",
        "        max_df=0.85,\n",
        "        min_df=2,\n",
        "        max_features=8000\n",
        "    )\n",
        "\n",
        "\n",
        "    # Lexicon features (the class returns a sparse matrix now), NOTE: used lexicons from https://gist.githubusercontent.com/mkulakowski2/4289437/raw/\n",
        "    lexicon_extractor = LexiconFeatureExtractor( #Lexicon features\n",
        "        pos_file='positive-words.txt',\n",
        "        neg_file='negative-words.txt'\n",
        "    ) #The files are available on the following webpage, under the section \"Opinion Lexicon\": https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html\n",
        "\n",
        "    # Small stylistic numeric features (length, ! count, ? count, uppercase ratio, url, mention)\n",
        "    class StyleFeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "        def fit(self, X, y=None):\n",
        "            return self\n",
        "        def transform(self, X):\n",
        "            out = []\n",
        "            for text in X:\n",
        "                length = len(text)\n",
        "                exclamation_marks = text.count('!')\n",
        "                question_marks = text.count('?')\n",
        "                upper_ratio = sum(1 for char in text if char.isupper()) / (length + 1)\n",
        "                has_url = 1 if ('http' in text or 'www.' in text) else 0\n",
        "                has_mention = 1 if '@' in text else 0\n",
        "                out.append([length, exclamation_marks, question_marks, upper_ratio, has_url, has_mention])\n",
        "                sparse_matrix = sparse.csr_matrix(np.array(out))\n",
        "            return sparse_matrix #convert to sparse to stack with TF-IDF and retuen sparse matrix\n",
        "\n",
        "    style_extractor = StyleFeatureExtractor()\n",
        "\n",
        "    feature_union = FeatureUnion([ #combining features using FeatureUnion\n",
        "        ('ngram_features', ngram_vectorizer),\n",
        "        ('char_features', char_vectorizer),\n",
        "        ('lexicon_features', lexicon_extractor),\n",
        "        ('style_features', style_extractor)\n",
        "    ], n_jobs = -1)\n",
        "\n",
        "    if train:\n",
        "        feature_union.fit(data) #for training, fit the union to the data and return it\n",
        "        return feature_union\n",
        "    else:\n",
        "        return feature_union.transform(data) #assumes 'feature_union' is a global or passed variable that was previously fitted in 'train_classifier'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jIpZ8CMajdiE"
      },
      "outputs": [],
      "source": [
        "def load_data(path):\n",
        "    \"\"\"Load data from a tab-separated file and append it to raw_data.\"\"\"\n",
        "    with open(path) as f:\n",
        "        reader = csv.reader(f, delimiter='\\t')\n",
        "        for line in reader:\n",
        "            (label, text) = parse_data_line(line)\n",
        "            raw_data.append((text, label))\n",
        "\n",
        "def split_and_preprocess_data(percentage):\n",
        "    \"\"\"Split the data between train_data and test_data according to the percentage\n",
        "    and performs the preprocessing.\"\"\"\n",
        "    num_samples = len(raw_data)\n",
        "    num_training_samples = int((percentage * num_samples))\n",
        "    for (text, label) in raw_data[:num_training_samples]:\n",
        "        train_data.append((text, label)) # Append raw text and label\n",
        "    for (text, label) in raw_data[num_training_samples:]:\n",
        "        test_data.append((text, label)) # Append raw text and label\n",
        "\n",
        "def parse_data_line(data_line):\n",
        "    \"\"\"Return a tuple of the label as just FAKE or REAL and the statement\"\"\"\n",
        "    return (data_line[1], data_line[2])\n",
        "\n",
        "def pre_process(text, lemmatizer, stop_words):\n",
        "    preprocessed_tokens = []\n",
        "    negation_tokens = ['not', 'no', 'never', \"n't\"]  # common negation words\n",
        "\n",
        "    # Normalizing and removing noisy tokens from texts\n",
        "    text = re.sub(r'\\bRT\\b', '', text) #remove RT tokens\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', ' URL ', text) #replace urls with placeholder\n",
        "    text = re.sub(r'@\\w+', ' MENTION ', text) #replace mentions with placeholder\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text) #collapse repeated characters (like \"soooo\" to \"soo\")\n",
        "    text = text.lower()     #lowercasing after placeholders and elongation normalization\n",
        "    # tidy up whitespace produced by replacements\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # 2) Tokenization: include punctuation tokens so we can use them to delimit negation scope\n",
        "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", text, flags=re.UNICODE)\n",
        "\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        token = tokens[i]\n",
        "\n",
        "        #Negation handling\n",
        "        if token in negation_tokens:\n",
        "            preprocessed_tokens.append(lemmatizer.lemmatize(token)) # keeping the negation token as it is\n",
        "            j = i + 1\n",
        "            steps = 0\n",
        "            while j < len(tokens) and steps < 3: #prefix following up to 3 tokens or until punctuation\n",
        "                next_token = tokens[j]\n",
        "                if re.match(r'[.!?,;:]', next_token): #stop negation handling scope when punctuation is reached\n",
        "                    break\n",
        "                # only add real word tokens (skip placeholders already added as MENTION/URL)\n",
        "                if next_token not in stop_words and re.match(r'\\w+', next_token):\n",
        "                    lemmatized_next = lemmatizer.lemmatize(next_token)\n",
        "                    preprocessed_tokens.append(f'NOT_{lemmatized_next}')\n",
        "                    steps += 1\n",
        "                j += 1\n",
        "            i = j  #continue after negation hangling scope\n",
        "            continue\n",
        "\n",
        "        #dropping stopwords and lemmatizing\n",
        "        if token not in stop_words and re.match(r'\\w+', token):\n",
        "            preprocessed_tokens.append(lemmatizer.lemmatize(token))\n",
        "        i += 1\n",
        "\n",
        "    return preprocessed_tokens #return list of tokens for TfidfVectorizer's tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGxSeAfHjdiE",
        "outputId": "88e70bdd-4fc9-4556-b648-47bcad727d67"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mention',\n",
              " 'another',\n",
              " 'bloody',\n",
              " 'instant',\n",
              " 'restaurant',\n",
              " 'week',\n",
              " 'seriously',\n",
              " 'jumped',\n",
              " 'shark',\n",
              " 'riding',\n",
              " 'two',\n",
              " 'shark',\n",
              " 'powered',\n",
              " 'sh']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# test\n",
        "text = \"RT @colonelkickhead: Another bloody instant restaurant week?!?! Seriously! They just jumped the shark riding two other sharks powered by sh…\"\n",
        "pre_process(text, lemmatizer, stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZVAa2YZxjdiF"
      },
      "outputs": [],
      "source": [
        "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "# Assuming 'to_feature_vector' is defined elsewhere and returns the FeatureUnion\n",
        "\n",
        "def train_classifier(train_data):\n",
        "    train_texts = [row[0] for row in train_data]\n",
        "    train_labels = [row[1] for row in train_data]\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    # Transform string labels ('negative', 'positive') to integers (e.g., 0, 1)\n",
        "    train_labels_encoded = le.fit_transform(train_labels)\n",
        "\n",
        "    # Calculate and sample weights for class imbalance\n",
        "    #calculate the weight for each class label\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=le.classes_,\n",
        "        y=train_labels # Use the original string labels here\n",
        "    )\n",
        "    #mapping the calculated weights to the encoded labels (0 and 1)\n",
        "    weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "    #create an array of sample weights, matching the length of the training data\n",
        "    sample_weights = np.array([weight_dict[label] for label in train_labels_encoded])\n",
        "\n",
        "    # Build the FeatureUnion fitted on training texts\n",
        "    # IMPORTANT: Ensure to_feature_vector can handle passing features to a tree-based model\n",
        "    # (i.e., the output from FeatureUnion must be compatible, which it should be since it's sparse/dense)\n",
        "    feature_union = to_feature_vector(train_texts, pre_process, train=True) # returns the fitted FeatureUnion object\n",
        "\n",
        "    # --- CLASSIFIER: XGBOOST ---\n",
        "    # XGBoost is excellent for leveraging your mix of sparse (TF-IDF) and dense (Lexicon/Style) features.\n",
        "    xgb_classifier = XGBClassifier(\n",
        "        objective='multi:softmax', #standard for multi-class classification\n",
        "        num_class=2,\n",
        "        eval_metric='merror', #metric to monitor: multi-class error rate\n",
        "        n_estimators=1000, #number of boosting rounds (trees) default: 200\n",
        "        learning_rate=0.01, #controls step size - tune this aggressively default: 0.1\n",
        "        max_depth=3, #tree depth - tune between 3 and 7 default: 3\n",
        "        reg_lambda=1.0, #L2 regularization rate\n",
        "        random_state=42,\n",
        "        #use_label_encoder=False, #suppress warning\n",
        "        # Calculate 'scale_pos_weight' based on the ratio of the majority class to the minority class\n",
        "        # (e.g., if negative samples are 2x positive samples, use 2.0).\n",
        "        # You will need to calculate this from your data. Using 1.0 here as a neutral placeholder.\n",
        "        # This is CRITICAL for balancing the F1 score.\n",
        "        #scale_pos_weight=1.0\n",
        "    )\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('features', feature_union),\n",
        "        ('classifier', xgb_classifier)\n",
        "    ])\n",
        "\n",
        "    #pipeline.fit(train_texts, train_labels) # training the pipeline\n",
        "    pipeline.fit(train_texts, train_labels_encoded, classifier__sample_weight=sample_weights) ## Use the step name 'classifier' followed by '__' and the parameter name 'sample_weight'\n",
        "\n",
        "    return pipeline, le\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cZ48zywYjdiG"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "# a function to make the confusion matrix readable and pretty\n",
        "def confusion_matrix_heatmap(y_test, preds, labels):\n",
        "    \"\"\"Function to plot a confusion matrix\"\"\"\n",
        "    # pass labels to the confusion matrix function to ensure right order\n",
        "    # cm = metrics.confusion_matrix(y_test, preds, labels)\n",
        "    cm = metrics.confusion_matrix(y_test, preds, labels=labels)\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(cm)\n",
        "    plt.title('Confusion matrix of the classifier')\n",
        "    fig.colorbar(cax)\n",
        "    ax.set_xticks(np.arange(len(labels)))\n",
        "    ax.set_yticks(np.arange(len(labels)))\n",
        "    ax.set_xticklabels( labels, rotation=45)\n",
        "    ax.set_yticklabels( labels)\n",
        "\n",
        "    for i in range(len(cm)):\n",
        "        for j in range(len(cm)):\n",
        "            text = ax.text(j, i, cm[i, j],\n",
        "                           ha=\"center\", va=\"center\", color=\"w\")\n",
        "\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "\n",
        "    # fix for mpl bug that cuts off top/bottom of seaborn viz:\n",
        "    b, t = plt.ylim() # discover the values for bottom and top\n",
        "    b += 0.5 # Add 0.5 to the bottom\n",
        "    t -= 0.5 # Subtract 0.5 from the top\n",
        "    plt.ylim(b, t) # update the ylim(bottom, top) values\n",
        "    plt.show() # ta-da!\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-70Ah-lzjdiF"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "\n",
        "def cross_validate(dataset, folds):\n",
        "    results = []\n",
        "    fold_size = int(len(dataset)/folds) + 1\n",
        "\n",
        "    for i in range(0,len(dataset),int(fold_size)):\n",
        "        print(\"Fold start on items %d - %d\" % (i, i+fold_size))\n",
        "        train_set = dataset[ : i] + dataset[i+fold_size : ] # the train set contains the remaining rows apart from the fold\n",
        "        test_set = dataset[i : i+fold_size] # the test set contains the rows within the fold\n",
        "\n",
        "        classifier, le = train_classifier(train_set)\n",
        "\n",
        "        test_data = [row[0] for row in test_set] #list comprehension to get all the text data in the test set\n",
        "        y_true = [row[1] for row in test_set] #list comprehension to get all the corresponding text labels in the test set\n",
        "\n",
        "        #checking to see if the dataset is imbalanced (It is imbalanced by observation)\n",
        "        #pos_counts = np.sum([1 for label in y_true if label == \"positive\"])\n",
        "        #neg_counts = np.sum([1 for label in y_true if label == \"negative\"])\n",
        "        #print(f\"No.of texts with positive sentiment: {pos_counts}\")\n",
        "        #print(f\"No.of texts with negative sentiment: {neg_counts}\")\n",
        "\n",
        "        #predictions are numeric (0 or 1)from XGBoost\n",
        "        y_pred_encoded = predict_labels(test_data, classifier)\n",
        "\n",
        "        #decode predictions back to strings for eval\n",
        "        y_pred = le.inverse_transform(y_pred_encoded)\n",
        "\n",
        "        #y_pred = predict_labels(test_data, classifier)\n",
        "\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "        #using weighted average to compute the metrics since dataset is imbalanced\n",
        "        precision, recall, f1_score, support = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
        "\n",
        "        results.append({\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1_score\": f1_score})\n",
        "        print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1_score}\")\n",
        "\n",
        "        '''Performing error analysis (as stated in Question #3) on the first fold'''\n",
        "        if i == 0:\n",
        "            print(\"---------------------------------------Error Analysis on the first fold (as per Question #3)-----------------------------------------------\")\n",
        "            labels = [\"positive\", \"negative\"]\n",
        "            confusion_matrix_heatmap(y_true, y_pred, labels)\n",
        "\n",
        "            #using list comprehension to identify the true positives and true negatives for the positive class in the first fold\n",
        "            true_positives = [test_data[j] for j in range(len(y_pred)) if y_pred[j] == \"positive\" and y_true[j] == \"positive\"]\n",
        "            true_negatives = [test_data[j] for j in range(len(y_pred)) if y_pred[j] == \"negative\" and y_true[j] == \"negative\"]\n",
        "\n",
        "            #using list comprehension to identify the false positives and false negatives for the positive class in the first fold\n",
        "            false_positives = [test_data[j] for j in range(len(y_pred)) if y_pred[j] == \"positive\" and y_true[j] == \"negative\"]\n",
        "            false_negatives = [test_data[j] for j in range(len(y_pred)) if y_pred[j] == \"negative\" and y_true[j] == \"positive\"]\n",
        "\n",
        "            print(\"Number of false positives: \",len(false_positives))\n",
        "            print(\"Number of false negatives: \", len(false_negatives))\n",
        "\n",
        "            batch_of_true_positives = true_positives[:5] #5 examples to just show\n",
        "            batch_of_true_negatives = true_negatives[:5] #5 examples to just show\n",
        "            batch_of_false_positives = false_positives[:5] #5 examples to just show\n",
        "            batch_of_false_negatives = false_negatives[:5] #5 examples to just show\n",
        "\n",
        "            print(\"\\n\")\n",
        "            print(\"First 5 examples of False Positives:\")\n",
        "            for false_pos in false_positives[:5]:\n",
        "                print(false_pos)\n",
        "\n",
        "            print(\"\\n\")\n",
        "            print(\"First 5 examples of False Negatives:\")\n",
        "            for false_neg in false_negatives[:5]:\n",
        "                print(false_neg)\n",
        "\n",
        "            print(\"\\n\")\n",
        "            print(\"First 5 examples of True Positives:\")\n",
        "            for false_pos in true_positives[:5]:\n",
        "                print(false_pos)\n",
        "\n",
        "            print(\"\\n\")\n",
        "            print(\"First 5 examples of True Negatives:\")\n",
        "            for false_neg in true_negatives[:5]:\n",
        "                print(false_neg)\n",
        "            print(\"--------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "    accuracies = [result[\"accuracy\"] for result in results]\n",
        "    precisions = [result[\"precision\"] for result in results]\n",
        "    recalls = [result[\"recall\"] for result in results]\n",
        "    f1_scores = [result[\"f1_score\"] for result in results]\n",
        "\n",
        "    cv_results = {\"accuracy\" : float(np.mean(accuracies)), \"precision\" : float(np.mean(precisions)),\n",
        "                  \"recall\" : float(np.mean(recalls)), \"f1_score\" : float(np.mean(f1_scores))}\n",
        "\n",
        "    return cv_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "csDEYYmSjdiF"
      },
      "outputs": [],
      "source": [
        "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
        "\n",
        "def predict_labels(samples, classifier):\n",
        "    \"\"\"Assuming preprocessed samples, return their predicted labels from the classifier model.\"\"\"\n",
        "    # Since the classifier is an sklearn Pipeline, use its predict method.\n",
        "    return classifier.predict(samples)\n",
        "\n",
        "def predict_label_from_raw(sample, classifier):\n",
        "    \"\"\"Assuming raw text, return its predicted label from the classifier model.\"\"\"\n",
        "    # Use pipeline.predict on a single raw sample after it is wrapped in a list\n",
        "    return classifier.predict([sample])[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhQWTSHejdiF",
        "outputId": "2710a0c2-6b3f-49f8-ee01-584b5eea32c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now 0 rawData, 0 trainData, 0 testData\n",
            "Preparing the dataset...\n",
            "Now 33540 rawData, 0 trainData, 0 testData\n",
            "Preparing training and test data...\n"
          ]
        }
      ],
      "source": [
        "# MAIN\n",
        "\n",
        "# loading reviews\n",
        "# initialize global lists that will be appended to by the methods below\n",
        "raw_data = []          # the filtered data from the dataset file\n",
        "train_data = []        # the pre-processed training data as a percentage of the total dataset\n",
        "test_data = []         # the pre-processed test data as a percentage of the total dataset\n",
        "\n",
        "\n",
        "# references to the data files\n",
        "data_file_path = 'sentiment-dataset.tsv'\n",
        "\n",
        "# Do the actual stuff (i.e. call the functions we've made)\n",
        "# We parse the dataset and put it in a raw data list\n",
        "print(\"Now %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
        "      \"Preparing the dataset...\",sep='\\n')\n",
        "\n",
        "load_data(data_file_path)\n",
        "\n",
        "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
        "# You do the cross validation on the 80% (training data)\n",
        "# We print the number of training samples and the number of features before the split\n",
        "print(\"Now %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
        "      \"Preparing training and test data...\",sep='\\n')\n",
        "\n",
        "split_and_preprocess_data(0.8)\n",
        "\n",
        "# We print the number of training samples and the number of features after the split\n",
        "#print(\"After split, %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
        "#      \"Training Samples: \", len(train_data), \"Features: \", len(global_feature_dict), sep='\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J-oLjBtVjdiF",
        "outputId": "19563374-000e-43dd-aa32-0fe05398ff62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold start on items 0 - 2684\n",
            "Accuracy: 0.8274962742175856, Precision: 0.8323874616914719, Recall: 0.8274962742175856, F1 Score: 0.8293620908495193\n",
            "---------------------------------------Error Analysis on the first fold (as per Question #3)-----------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of false positives:  197\n",
            "Number of false negatives:  266\n",
            "\n",
            "\n",
            "First 5 examples of False Positives:\n",
            "@AngryRaiderFan I know. This, TPP, expanded wars and drone strikes, mass surveillance, on and on...\n",
            "@RichardDawkins  Just like Christians with their bible, Americans are cherry picking the 2nd amendment.  #gunsense\n",
            "Matt Flynn May Never Play Quarterback for the Seattle Seahawks - Bleacher Report: Bleacher Report... http://t.co/8HuFaY8L #seahawks #nfl\n",
            "@nytimes And in good news tonight, Fidel Castro assumed room temperature. Now we just need his brother and Maduro to follow. Quickly.\n",
            "@TimesOfIndia btw how much money you charged from \"eminent people\" to make Yakub's headline a prominent one ??? https://t.co/z586zvUrsA\n",
            "\n",
            "\n",
            "First 5 examples of False Negatives:\n",
            "@SegmentNext I got an xbox one on the 7th of April this year and it looks good for zombie games\n",
            "@Jacknonce Sorry, the blacks I know are actually great, successful people.They aren't taken in by Dems BS. They are very smart, educated!\n",
            "@aalsi_aadmi @Sab_mile_Hue you must also do some learning in Islam and you may get content to your reason and logic.\n",
            "I have heard it said that the 2nd G in Snoop Dogg represents extra gangsterishness, for how else do you explain this particular redundancy?\n",
            "Let me be the 1st to say it... you people are about to be all over this Weeknd album. Especially the Dark Times song w/ Ed Sheeran\n",
            "\n",
            "\n",
            "First 5 examples of True Positives:\n",
            "Literally so excited I'm going to a Sam Smith concert in October\n",
            "@rinashah I have been using Moto G 2nd Gen for over a month now and it's an absolute delight. Stock Android. Good design. Best.\n",
            "cried for every episode of Dream High 2 starting from episode 13!!! T-T tomorrow i shall watch the last and final episode!\n",
            "Happy birthday gay, see you Monday. IMMA FUCKING JUMP ON YOU OKAY! Have a good day my love, love you lots @_SophieeWhite_\n",
            "@MedievalBex Oh! Wow! I must see it then... Daniel Radcliffe is not one of my fave actors, but this may help me overcome my aversion... :D x\n",
            "\n",
            "\n",
            "First 5 examples of True Negatives:\n",
            "Juan  Just heard Green Day's 'Time of our life' for the 1st time since leaving florida and i burst into tears. I miss everyone...  Kellogg\n",
            "So Fidel Castro has died.  Don't worry, George Soros is willing to fill his shoes as Most Wicked Man In The World #wicked #publicenemy1\n",
            "@CheleReamey @funder @ZeldaShagnasty @FBI @TheJusticeDept yes I know but Comey should be pressured for further explanation about letter .\n",
            "Shout out to @ZombeaversMovie for being the worst movie since @SharknadoSyfy... Sharknado has a chance to reclaim the title tomorrow...\n",
            "The most conclusive argument against the bleeding hearts and why logic doesn't hold for them in Yakub's hanging. https://t.co/RrlAj1K9aN\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Fold start on items 2684 - 5368\n",
            "Accuracy: 0.8360655737704918, Precision: 0.8474003785610935, Recall: 0.8360655737704918, F1 Score: 0.8404313690548159\n",
            "Fold start on items 5368 - 8052\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-712632039.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will work and output overall performance of p, r, f-score when cv implemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-183258762.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(dataset, folds)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfold_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# the test set contains the rows within the fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#list comprehension to get all the text data in the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3813590817.py\u001b[0m in \u001b[0;36mtrain_classifier\u001b[0;34m(train_data)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m#pipeline.fit(train_texts, train_labels) # training the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier__sample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## Use the step name 'classifier' followed by '__' and the parameter name 'sample_weight'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0mrouted_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_method_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouted_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pipeline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, routed_params, raw_params)\u001b[0m\n\u001b[1;32m    586\u001b[0m             )\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    589\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1549\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m             res = transformer.fit(X, y, **params.get(\"fit\", {})).transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1972\u001b[0m                     \u001b[0mrouted_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1974\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parallel_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fit_transform_one\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouted_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1975\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1976\u001b[0m             \u001b[0;31m# All transformers are None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_parallel_func\u001b[0;34m(self, X, y, func, routed_params)\u001b[0m\n\u001b[1;32m   1994\u001b[0m         \u001b[0mtransformers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1996\u001b[0;31m         return Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[1;32m   1997\u001b[0m             delayed(func)(\n\u001b[1;32m   1998\u001b[0m                 \u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "cross_validate(train_data, 10)  # will work and output overall performance of p, r, f-score when cv implemented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4eR6YYpjdiG",
        "outputId": "4e8785a6-390e-4f13-d143-85aa8fb8d68b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"Tomorrow we'll release our 58th episode of #HSonAir profiling our very own @AlissaDosSantos ! We'll talk about storytelling and Beyonce!\", 'positive')\n"
          ]
        }
      ],
      "source": [
        "# Finally, check the accuracy of your classifier by training on all the traning data\n",
        "# and testing on the test set\n",
        "# Will only work once all functions are complete\n",
        "functions_complete = True  # set to True once you're happy with your methods for cross val\n",
        "if functions_complete:\n",
        "    print(test_data[0])   # have a look at the first test data instance\n",
        "    classifier = train_classifier(train_data)  # train the classifier\n",
        "    test_true = [t[1] for t in test_data]   # get the ground-truth labels from the data\n",
        "    test_pred = predict_labels([x[0] for x in test_data], classifier)  # classify the test data to get predicted labels\n",
        "    final_scores = precision_recall_fscore_support(test_true, test_pred, average='weighted') # evaluate\n",
        "    print(\"Done training!\")\n",
        "    print(\"Precision: %f\\nRecall: %f\\nF Score:%f\" % final_scores[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFkVQwtkjdiG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFgbbH72jdiG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V6E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}